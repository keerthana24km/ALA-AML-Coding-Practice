{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "R5VPHPs-tRlh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dEgRpy3952M"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a function of one independent variable:\n",
        "$$L = (w-10)^3 + 2w.$$"
      ],
      "metadata": {
        "id": "7HE2LfpvtWts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's look at a function of one independent variable\n",
        "#L = lambda w: (w-10)**3 + 2*w\n",
        "L = lambda w: (w-10)**2 + 2*w\n",
        "fig, ax = plt.subplots(1, 1, figsize = (4, 4))\n",
        "w = np.arange(-30, 30, 1e-03)\n",
        "ax.plot(w, L(w), 'k-')\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('L(w)')"
      ],
      "metadata": {
        "id": "OQhQkhhucUeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent in 1D"
      ],
      "metadata": {
        "id": "tANE7j4ktkr8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkgAj28XNDY0"
      },
      "source": [
        "## Gradient descent in 1D\n",
        "L = lambda w: w**2 + 3\n",
        "gradL  = lambda w: 2*w\n",
        "\n",
        "# Try 1e-05 (slow learning rate), 1e-01 (optimal),\n",
        "# 1e0 (oscillates and does not converge),\n",
        "# and 0.95 (oscillates towards the end and converges)\n",
        "alpha = 1e-01 # learning rate (or) step size\n",
        "tol = 1e-05 # stopping tolerance\n",
        "iter = 0\n",
        "maxiter = 1000\n",
        "\n",
        "w = 1 # starting point\n",
        "\n",
        "# Learning process\n",
        "while np.abs(gradL(w)) > tol and iter < maxiter:\n",
        "  w = w + alpha * -gradL(w)\n",
        "  iter = iter+1\n",
        "  print('Iteration = %d, w = %f, gradL(w) = %f'%(iter, w, gradL(w)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent in 2D for a function of two variables: $$L(w_1, w_2) = (w_1-2)^2+(w_1+3)^2.$$"
      ],
      "metadata": {
        "id": "5B_lYwK8uJjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Gradient descent in 2D\n",
        "L = lambda w: (w[0]-2)**2 + (w[1]+3)**2\n",
        "gradL = lambda w: np.array([2*(w[0]-2), 2*(w[1]+3)])\n",
        "alpha = 1e1 # learning rate (or) step size\n",
        "tol = 1e-05 # stopping tolerance\n",
        "iter = 0\n",
        "maxiter = 1000\n",
        "\n",
        "w =  np.array([1, 1]) # initial guess\n",
        "\n",
        "while np.linalg.norm(gradL(w)) > tol and iter < maxiter:\n",
        "  w = w + alpha *(-gradL(w))\n",
        "  iter = iter+1\n",
        "  print('Iteration = %d, w1 = %f, w2 = %f, ||gradL(w)|| = %f'%(iter, w[0], w[1], np.linalg.norm(gradL(w))))"
      ],
      "metadata": {
        "id": "CpcTS8SYuG-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}